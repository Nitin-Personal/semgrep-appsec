name: v2- Semgrep Coverage Report

on:
  # Scan on-demand through GitHub Actions interface:
  workflow_dispatch: {}
  # Schedule the CI job (this method uses cron syntax):

jobs:
  get-list-of-repos:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        pip install requests
        pip install pandas
        
    - name: Get list of repositories
      env:
        # Generate PAT with Read-Only access to all repos in your GH ORG
        PAT_READ_ONLY_CUSTOMER_REPO: ${{ secrets.PAT_READ_ONLY_CUSTOMER_REPO }}
        SEMGREP_API_WEB_TOKEN:  ${{ secrets.SEMGREP_API_WEB_TOKEN }}
        SEMGREP_DEPLOYMENT_SLUG: ${{ vars.SEMGREP_DEPLOYMENT_SLUG }}

      run: |
        import requests
        import os
        import pandas as pd
        import json
        import sys
        import logging

        # Set up logging
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

        # GitHub repository in the format "org_name/repo_name"
        full_repo_name = os.environ['GITHUB_REPOSITORY']
        # Extract the organization name
        org_name = full_repo_name.split('/')[0]
        logging.info(f"Organization Name: {org_name}")
        pat = os.environ['PAT_READ_ONLY_CUSTOMER_REPO']
        headers = {'Authorization': f'token {pat}'}

        repos = []
        page = 1
        while True:
            url = f'https://api.github.com/orgs/{org_name}/repos?page={page}&per_page=100'
            response = requests.get(url, headers=headers)
            logging.debug(f"Requesting list of repos in Org: {org_name}, page: {page}")
            logging.debug(f"Response Code: {response.status_code}")
            page_repos = response.json()
            if not page_repos:
                break
            repos.extend(page_repos)
            page += 1

        daily_df = pd.read_csv('daily.csv', header=None)
        weekly_df = pd.read_csv('weekly.csv', header=None)
        
        daily_repos = daily_df[0].tolist()
        weekly_repos = weekly_df[0].tolist()
        github_repos = [repo['name'] for repo in repos]

        daily_repos_not_in_github_repos = [item for item in daily_repos if item not in github_repos]
        weekly_repos_not_in_github_repos = [item for item in weekly_repos if item not in github_repos]

        df_daily_repos_not_in_github_repos = pd.DataFrame(daily_repos_not_in_github_repos)
        df_weekly_repos_not_in_github_repos = pd.DataFrame(weekly_repos_not_in_github_repos)

        df_daily_repos_not_in_github_repos.to_csv('daily_repos_not_in_github_repos.csv', index=False)
        df_weekly_repos_not_in_github_repos.to_csv('weekly_repos_not_in_github_repos.csv', index=False)

        headers = {"Accept": "application/json", "Authorization": "Bearer " + os.environ['SEMGREP_API_WEB_TOKEN']}
        params =  {"page_size": 3000}

        r = requests.get(f"https://semgrep.dev/api/v1/deployments/{os.environ['SEMGREP_DEPLOYMENT_SLUG']}/projects?page=0", params=params, headers=headers)
        if r.status_code != 200:
            logging.error("Getting list of projects from Semgrep failed - please check Semgrep API token")
            sys.exit(f'Get failed: {r.text}')
        
        logging.debug("Getting list of projects from Semgrep successful")
        
        semgrep_repos = json.loads(r.text)

        coverage_data = []
        
        for repo in repos:
            repo_name = repo['name']
            repo_full_name = repo['full_name']
            created_at = repo['created_at']
            html_url = repo['html_url']
            
            latest_scan = None
            for sg_repo in semgrep_repos['projects']:
                if sg_repo['name'] == repo_full_name:
                    latest_scan = sg_repo['latest_scan_at']
                    break
            
            if latest_scan is None:
                logging.info(f"{repo_name} not scanned by Semgrep yet")
            
            daily_mark = 1 if repo_name in daily_repos else 0
            weekly_mark = 1 if repo_name in weekly_repos else 0
            not_covered = 1 if (daily_mark + weekly_mark == 0) else 0
            duplicate_scans = 1 if (daily_mark + weekly_mark == 2) else 0
            
            coverage_data.append([repo_name, daily_mark, weekly_mark, not_covered, duplicate_scans, latest_scan, created_at, html_url])
        
        coverage_df = pd.DataFrame(coverage_data, columns=['Repository', 'Daily', 'Weekly', 'Not Covered', 'Duplicate Scans', 'Latest Scan', 'Created At', 'URL'])
        coverage_df.to_csv('coverage.csv', index=False)
      
      shell: python
      
    - name: Upload coverage.CSV as Artifact
      uses: actions/upload-artifact@v2
      with:
        name: semgrep-coverage-report.csv
        path: |
          daily_repos_not_in_github_repos.csv
          weekly_repos_not_in_github_repos.csv
          coverage.csv
